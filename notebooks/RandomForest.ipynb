{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing different features with RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "\n",
    "# Caching stopwords\n",
    "nltk.download('stopwords', quiet=True)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "from fact_classification import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence_id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Speaker_title</th>\n",
       "      <th>Speaker_party</th>\n",
       "      <th>File_id</th>\n",
       "      <th>Length</th>\n",
       "      <th>Line_number</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Verdict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>I think we've seen a deterioration of values.</td>\n",
       "      <td>George Bush</td>\n",
       "      <td>Vice President</td>\n",
       "      <td>REPUBLICAN</td>\n",
       "      <td>1988-09-25.txt</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>I think for a while as a nation we condoned th...</td>\n",
       "      <td>George Bush</td>\n",
       "      <td>Vice President</td>\n",
       "      <td>REPUBLICAN</td>\n",
       "      <td>1988-09-25.txt</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>-0.456018</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18</td>\n",
       "      <td>For a while, as I recall, it even seems to me ...</td>\n",
       "      <td>George Bush</td>\n",
       "      <td>Vice President</td>\n",
       "      <td>REPUBLICAN</td>\n",
       "      <td>1988-09-25.txt</td>\n",
       "      <td>29</td>\n",
       "      <td>18</td>\n",
       "      <td>-0.805547</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19</td>\n",
       "      <td>So we've seen a deterioration in values, and o...</td>\n",
       "      <td>George Bush</td>\n",
       "      <td>Vice President</td>\n",
       "      <td>REPUBLICAN</td>\n",
       "      <td>1988-09-25.txt</td>\n",
       "      <td>35</td>\n",
       "      <td>19</td>\n",
       "      <td>0.698942</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>We got away, we got into this feeling that val...</td>\n",
       "      <td>George Bush</td>\n",
       "      <td>Vice President</td>\n",
       "      <td>REPUBLICAN</td>\n",
       "      <td>1988-09-25.txt</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentence_id                                               Text   \n",
       "0           16      I think we've seen a deterioration of values.  \\\n",
       "1           17  I think for a while as a nation we condoned th...   \n",
       "2           18  For a while, as I recall, it even seems to me ...   \n",
       "3           19  So we've seen a deterioration in values, and o...   \n",
       "4           20  We got away, we got into this feeling that val...   \n",
       "\n",
       "       Speaker   Speaker_title Speaker_party         File_id  Length   \n",
       "0  George Bush  Vice President    REPUBLICAN  1988-09-25.txt       8  \\\n",
       "1  George Bush  Vice President    REPUBLICAN  1988-09-25.txt      16   \n",
       "2  George Bush  Vice President    REPUBLICAN  1988-09-25.txt      29   \n",
       "3  George Bush  Vice President    REPUBLICAN  1988-09-25.txt      35   \n",
       "4  George Bush  Vice President    REPUBLICAN  1988-09-25.txt      15   \n",
       "\n",
       "   Line_number  Sentiment  Verdict  \n",
       "0           16   0.000000       -1  \n",
       "1           17  -0.456018       -1  \n",
       "2           18  -0.805547       -1  \n",
       "3           19   0.698942       -1  \n",
       "4           20   0.000000       -1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df, df_crowdsourced, df_ground_truth = data_loading(local=True)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the baseline model scoring results. We will use this to compare against our new models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>algorithm</th>\n",
       "      <th>features</th>\n",
       "      <th>p_NFS</th>\n",
       "      <th>p_UFS</th>\n",
       "      <th>p_CFS</th>\n",
       "      <th>p_wavg</th>\n",
       "      <th>r_NFS</th>\n",
       "      <th>r_UFS</th>\n",
       "      <th>r_CFS</th>\n",
       "      <th>r_wavg</th>\n",
       "      <th>f_NFS</th>\n",
       "      <th>f_UFS</th>\n",
       "      <th>f_CFS</th>\n",
       "      <th>f_wavg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RFC</td>\n",
       "      <td>W</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DUM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  algorithm features  p_NFS  p_UFS  p_CFS  p_wavg  r_NFS  r_UFS  r_CFS   \n",
       "0       RFC        W   0.67    0.6   0.81    0.70   0.99   0.06   0.23  \\\n",
       "0       DUM      NaN   0.62    0.0   0.00    0.38   1.00   0.00   0.00   \n",
       "\n",
       "   r_wavg  f_NFS  f_UFS  f_CFS  f_wavg  \n",
       "0    0.68   0.80   0.11   0.35    0.60  \n",
       "0    0.62   0.76   0.00   0.00    0.47  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load scoring results dataframe from the baseline model\n",
    "df_score_train, df_score_test = score_loading()\n",
    "df_score_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the features matrix that we generated in the `feature_generation.ipynb` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Pandas requires version '0.6.3' or newer of 'fastparquet' (version '0.5.0' currently installed).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_features \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_parquet(\u001b[39m'\u001b[39;49m\u001b[39m../results/features.gzip\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      2\u001b[0m df_features\n",
      "File \u001b[0;32m~/anaconda3/envs/fact_check/lib/python3.10/site-packages/pandas/io/parquet.py:493\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, **kwargs)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[39m@doc\u001b[39m(storage_options\u001b[39m=\u001b[39m_shared_docs[\u001b[39m\"\u001b[39m\u001b[39mstorage_options\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    429\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_parquet\u001b[39m(\n\u001b[1;32m    430\u001b[0m     path: FilePath \u001b[39m|\u001b[39m ReadBuffer[\u001b[39mbytes\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    437\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame:\n\u001b[1;32m    438\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \u001b[39m    Load a parquet object from the file path, returning a DataFrame.\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[39m    DataFrame\u001b[39;00m\n\u001b[1;32m    492\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 493\u001b[0m     impl \u001b[39m=\u001b[39m get_engine(engine)\n\u001b[1;32m    495\u001b[0m     \u001b[39mif\u001b[39;00m use_nullable_dtypes \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m lib\u001b[39m.\u001b[39mno_default:\n\u001b[1;32m    496\u001b[0m         msg \u001b[39m=\u001b[39m (\n\u001b[1;32m    497\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mThe argument \u001b[39m\u001b[39m'\u001b[39m\u001b[39muse_nullable_dtypes\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is deprecated and will be removed \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    498\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39min a future version.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    499\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/fact_check/lib/python3.10/site-packages/pandas/io/parquet.py:60\u001b[0m, in \u001b[0;36mget_engine\u001b[0;34m(engine)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m     58\u001b[0m             error_msgs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m - \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(err)\n\u001b[0;32m---> 60\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m     61\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnable to find a usable engine; \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     62\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtried using: \u001b[39m\u001b[39m'\u001b[39m\u001b[39mpyarrow\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mfastparquet\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     63\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mA suitable version of \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     64\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpyarrow or fastparquet is required for parquet \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     65\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msupport.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     66\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTrying to import the above resulted in these errors:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     67\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00merror_msgs\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     68\u001b[0m     )\n\u001b[1;32m     70\u001b[0m \u001b[39mif\u001b[39;00m engine \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpyarrow\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     71\u001b[0m     \u001b[39mreturn\u001b[39;00m PyArrowImpl()\n",
      "\u001b[0;31mImportError\u001b[0m: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Pandas requires version '0.6.3' or newer of 'fastparquet' (version '0.5.0' currently installed)."
     ]
    }
   ],
   "source": [
    "df_features = pd.read_parquet('../results/features.gzip')\n",
    "df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features_sparse = df_features.to_sparse()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForest with only Length and Sentiment features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Sentiment and Length because these have the highest correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = test_train_split(df)\n",
    "\n",
    "# PredefinedSplit index for GridSearchCV\n",
    "test_fold = np.append(\n",
    "    np.full((len(df_train),), -1, dtype=int),\n",
    "    np.full((len(df_test),), 0, dtype=int)\n",
    ")\n",
    "test_fold = PredefinedSplit(test_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    class_weight=\"balanced_subsample\",\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [5, 10, 20],\n",
    "    'criterion': ['gini', 'log_loss'],\n",
    "    'max_depth': [5, 10, 20, None],\n",
    "    'class_weight': ['balanced', 'balanced_subsample', None]\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(rfc, param_grid, cv=test_fold, n_jobs=-1)\n",
    "clf.fit(df_features_sparse, df['Verdict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pred_train, pred_test = predict_it(\n",
    "    df_train[['Length', 'Sentiment']],\n",
    "    df_train.Verdict,\n",
    "    df_test[['Length', 'Sentiment']],\n",
    "    method=method\n",
    ")\n",
    "\n",
    "df_score_test = pd.concat([\n",
    "    df_score_test,\n",
    "    score_it(\n",
    "        df_test.Verdict,\n",
    "        pred_test,\n",
    "        algorithm='RFC',\n",
    "        features='SL')]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "df_score_train = pd.concat([\n",
    "    df_score_train,\n",
    "    score_it(\n",
    "        df_train.Verdict,\n",
    "        pred_train,\n",
    "        algorithm='RFC',\n",
    "        features='SL')]\n",
    ").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using only Sentiment and Length gets very low accuracy, to not overfit the max depth of the random forrest is set to 5. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForest with POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = test_train_split(df_features.join(df[['File_id', 'Verdict']]))\n",
    "col_idx = df_train.columns.str.startswith('P_')\n",
    "\n",
    "method = RandomForestClassifier(\n",
    "    max_depth=5,\n",
    "    random_state=42,\n",
    "    class_weight=\"balanced_subsample\",\n",
    ")\n",
    "\n",
    "pred_train, pred_test = predict_it(\n",
    "    df_train.loc[:, col_idx],\n",
    "    df_train['Verdict'],\n",
    "    df_test.loc[:, col_idx],\n",
    "    method=method\n",
    ")\n",
    "\n",
    "df_score_test = pd.concat([\n",
    "    df_score_test,\n",
    "    score_it(\n",
    "        df_test['Verdict'],\n",
    "        pred_test,\n",
    "        algorithm='RFC',\n",
    "        features='P')]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "df_score_train = pd.concat([\n",
    "    df_score_train,\n",
    "    score_it(\n",
    "        df_train['Verdict'],\n",
    "        pred_train,\n",
    "        algorithm='RFC',\n",
    "        features='P')]\n",
    ").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better than just using sentiment and length, but barely better than the baseline model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df_train.loc[:, col_idx]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.loc[:, 'Verdict'] = df_train['Verdict'].values\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.columns[a[a.columns].corr()['Verdict'].abs() > 0.15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[a.columns].corr()['Verdict'].abs().sort_values(ascending = False)[1:10]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most predictive categories are \"vbd, vb, cd, nnp, vbp, prp, md, and in\". These represent verbs, digits, proper nouns, personal noun, modal, and preposition. This seems logical. The least predictive categories are \"rbs, ex, fw, uh, rbr\" they are the adverbs, existential, foreign words, and interjections. Again this makes logical sense, interjections like \"hmm\" and \"erm\" are probably more person dependent and less dependent on the type of sentence. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining methods (POS-tagging, sentiment and length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train = pd.DataFrame(train_tfid.toarray(), columns = vocabulary)\n",
    "pos_test = pd.DataFrame(test_tfid.toarray(), columns = vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.reset_index(inplace = True, drop = True)\n",
    "pos_train.reset_index(inplace = True, drop = True)\n",
    "df_test.reset_index(inplace = True, drop = True)\n",
    "pos_test.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_pos = df_train.join(pos_train)\n",
    "df_test_pos = df_test.join(pos_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cols = ['Length', 'Sentiment', 'cc', 'cd', 'dt', 'ex', 'fw', 'in', 'jj', 'jjr',\n",
    "       'jjs', 'md', 'nn', 'nnp', 'nnps', 'nns', 'pdt', 'prp', 'rb', 'rbr',\n",
    "       'rbs', 'rp', 'to', 'uh', 'vb', 'vbd', 'vbg', 'vbn', 'vbp', 'vbz', 'wdt',\n",
    "       'wp', 'wrb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = method=RandomForestClassifier(max_depth = 5,\n",
    "        random_state = 42,\n",
    "        class_weight = \"balanced_subsample\",\n",
    "    )\n",
    "pred_train, pred_test = predict_it(df_train_pos[train_cols], df_train_pos.Verdict, df_test_pos[train_cols], method = method)\n",
    "df_score_test = pd.concat([df_score_test, score_it(df_test_pos.Verdict, pred_test, features = 'Sentiment, Length, POS')]).reset_index(drop=True)\n",
    "df_score_train = pd.concat([df_score_train, score_it(df_train_pos.Verdict, pred_train, features = 'Sentiment, Length, POS')]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No big improvement with this"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining methods (pos tagging and stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForest Named Entity Recognition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `Spacy` package to generate the NER labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ner_tag'] = ner_labels(df['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['pos_tag', 'ner_tag']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pos_ner_combined'] = df['pos_tag'] + df['ner_tag'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = test_train_split(df)\n",
    "\n",
    "train_tfid, test_tfid, vocabulary = tfid(\n",
    "    train=df_train['pos_ner_combined'],\n",
    "    test=df_test['pos_ner_combined'],\n",
    "    n_gram_range=1\n",
    ")\n",
    "\n",
    "method = RandomForestClassifier(\n",
    "    max_depth=7,\n",
    "    random_state=42,\n",
    "    class_weight=\"balanced_subsample\",\n",
    ")\n",
    "\n",
    "pred_train, pred_test = predict_it(\n",
    "    train_tfid,\n",
    "    df_train.Verdict,\n",
    "    test_tfid,\n",
    "    method=method\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score_test = pd.concat([\n",
    "    df_score_test,\n",
    "    score_it(\n",
    "        df_test.Verdict,\n",
    "        pred_test,\n",
    "        algorithm='Random Forest',\n",
    "        features='POS+NER')]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "df_score_train = pd.concat([\n",
    "    df_score_train,\n",
    "    score_it(\n",
    "        df_train.Verdict,\n",
    "        pred_train,\n",
    "        algorithm='Random Forest',\n",
    "        features='POS+NER')]\n",
    ").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that we have a slight improvement in weighted f-score when we combine the POS+NER labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame(train_tfid.toarray(), columns = vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a['Verdict'] = df_train.Verdict.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.columns[a[a.columns].corr()['Verdict'].abs() > 0.15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[a.columns].corr()['Verdict'].abs().sort_values(ascending = False)[1:15]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have four NER labels with correlation above 0.15; money, date, percent, and cardinal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train = pd.DataFrame(train_tfid.toarray(), columns = vocabulary)\n",
    "pos_test = pd.DataFrame(test_tfid.toarray(), columns = vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_latex(df_score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
