{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import scipy as sp\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "from fact_classification import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datafiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, df_crowdsourced, df_ground_truth = data_loading(local=True)\n",
    "df['Sentiment'] = df.Sentiment.fillna(df.Sentiment[df.Verdict == -1].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load features\n",
    "Load the features matrix that we generated in the `feature_generation.ipynb` notebook. This is a large sparse matrix so ww convert it to Compressed Sparse Row (CSR) format to avoid running out of memory when fitting our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with bz2.open('../results/df_features.bz2') as f:\n",
    "    df_features = pickle.load(f)\n",
    "\n",
    "# Convert to compressed sparse row matrix\n",
    "# X = sp.sparse.csr_matrix(df_features)\n",
    "X = df_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data and generate indexes\n",
    "\n",
    "We split the dataset according to the instructions in the assignment, where data up until and including year 2008 will be used for training, and data after 2008 will be used for testing. Here we also generate indexes for the various feature sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test, idx_train = test_train_split(df)\n",
    "\n",
    "y = df['Verdict']\n",
    "y_train = df_train['Verdict'].to_numpy()\n",
    "y_test = df_test['Verdict'].to_numpy()\n",
    "\n",
    "X_train = X[idx_train].to_numpy()\n",
    "X_test = X[~idx_train].to_numpy()\n",
    "\n",
    "# Column index for the numeric columns Sentiment and Length\n",
    "col_idx_n = (df_features.columns == 'Sentiment') | (df_features.columns == 'Length')\n",
    "\n",
    "# Column index for TF-IDF features on the raw Text column with n-grams=1\n",
    "col_idx_w1 = df_features.columns.str.startswith('W1_')\n",
    "\n",
    "# Column index for TF-IDF features on the raw Text column with n-grams=2\n",
    "col_idx_w2 = df_features.columns.str.startswith('W2_')\n",
    "\n",
    "# Column index for TF-IDF features on the stemmed text with n-grams=1\n",
    "col_idx_ws = df_features.columns.str.startswith('WS_')\n",
    "\n",
    "# Column index for POS features\n",
    "col_idx_p = df_features.columns.str.startswith('P_')\n",
    "\n",
    "# Column index for NER labels\n",
    "col_idx_e = df_features.columns.str.startswith('E_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiments\n",
    "experiments = {\n",
    "    'N': col_idx_n,\n",
    "    'W': col_idx_w1,\n",
    "    'P': col_idx_p,\n",
    "    'E': col_idx_e,\n",
    "    'N_W': col_idx_n | col_idx_w1,\n",
    "    'N_P': col_idx_n | col_idx_p,\n",
    "    'N_E': col_idx_n | col_idx_e,\n",
    "    'N_W_P': col_idx_n | col_idx_w1 | col_idx_p,\n",
    "    'N_W_E': col_idx_n | col_idx_w1 | col_idx_e,\n",
    "    'N_W_P_E': col_idx_n | col_idx_w1 | col_idx_p | col_idx_e\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create PyTorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch datasets and loaders\n",
    "train_dataset = TensorDataset(torch.tensor(X_train[:, experiments['N_W_P_E']], dtype=torch.float32), torch.tensor(y_train + 1, dtype=torch.long))\n",
    "test_dataset = TensorDataset(torch.tensor(X_test[:, experiments['N_W_P_E']], dtype=torch.float32), torch.tensor(y_test + 1, dtype=torch.long))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the CNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the CNN classifier\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_size, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=2, stride=1, padding=1)\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add a dummy dimension\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = X_train[:, experiments['N_W_P_E']].shape[1]\n",
    "num_classes = 3\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "# Initialize the model\n",
    "model = CNNClassifier(input_size, num_classes).to(device)\n",
    "\n",
    "# Calculate class weights\n",
    "unique_classes = np.unique(y_train + 1)\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=unique_classes, y=y_train + 1)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.6025\n",
      "Epoch [2/10], Loss: 0.4329\n",
      "Epoch [3/10], Loss: 0.3607\n",
      "Epoch [4/10], Loss: 0.4842\n",
      "Epoch [5/10], Loss: 0.4011\n",
      "Epoch [6/10], Loss: 0.2103\n",
      "Epoch [7/10], Loss: 0.0808\n",
      "Epoch [8/10], Loss: 0.2602\n",
      "Epoch [9/10], Loss: 0.3206\n",
      "Epoch [10/10], Loss: 0.2910\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, labels) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>algorithm</th>\n",
       "      <th>features</th>\n",
       "      <th>p_NFS</th>\n",
       "      <th>p_UFS</th>\n",
       "      <th>p_CFS</th>\n",
       "      <th>p_wavg</th>\n",
       "      <th>r_NFS</th>\n",
       "      <th>r_UFS</th>\n",
       "      <th>r_CFS</th>\n",
       "      <th>r_wavg</th>\n",
       "      <th>f_NFS</th>\n",
       "      <th>f_UFS</th>\n",
       "      <th>f_CFS</th>\n",
       "      <th>f_wavg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN</td>\n",
       "      <td>N_W_P_E</td>\n",
       "      <td>0.807</td>\n",
       "      <td>0.309</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0.695</td>\n",
       "      <td>0.768</td>\n",
       "      <td>0.467</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.672</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.569</td>\n",
       "      <td>0.681</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  algorithm features  p_NFS  p_UFS  p_CFS  p_wavg  r_NFS  r_UFS  r_CFS   \n",
       "0       CNN  N_W_P_E  0.807  0.309  0.604   0.695  0.768  0.467  0.538  \\\n",
       "\n",
       "   r_wavg  f_NFS  f_UFS  f_CFS  f_wavg  \n",
       "0   0.672  0.787  0.372  0.569   0.681  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, labels in test_loader:\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(data)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "predictions = np.array(predictions) - 1\n",
    "true_labels = np.array(true_labels) - 1\n",
    "\n",
    "df_score_test = score_it(true_labels, predictions, features='N_W_P_E', algorithm='CNN')\n",
    "df_score_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export results to LaTeX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llrrrrrrrrrrrr}\n",
      "\\toprule\n",
      "algorithm & features & p\\_NFS & p\\_UFS & p\\_CFS & p\\_wavg & r\\_NFS & r\\_UFS & r\\_CFS & r\\_wavg & f\\_NFS & f\\_UFS & f\\_CFS & f\\_wavg \\\\\n",
      "\\midrule\n",
      "CNN & N\\_W\\_P\\_E & 0.807 & 0.309 & 0.604 & 0.695 & 0.768 & 0.467 & 0.538 & 0.672 & 0.787 & 0.372 & 0.569 & 0.681 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "to_latex(df_score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fact_check",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
