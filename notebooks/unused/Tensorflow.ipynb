{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-18 08:24:50.271313: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import bz2\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import scipy as sp\n",
    "\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "from fact_classification import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, df_crowdsourced, df_ground_truth = data_loading(local=True)\n",
    "df['Sentiment'] = df.Sentiment.fillna(df.Sentiment[df.Verdict == -1].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with bz2.open('../results/df_features.bz2') as f:\n",
    "    X = pickle.load(f)\n",
    "\n",
    "# Convert to compressed sparse row matrix\n",
    "# X = sp.sparse.csr_matrix(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test, idx_train = test_train_split(df)\n",
    "\n",
    "y = df['Verdict']\n",
    "y_train = df_train['Verdict']\n",
    "y_test = df_test['Verdict']\n",
    "\n",
    "X_train = X[idx_train]\n",
    "X_test = X[~idx_train]\n",
    "\n",
    "# Column index for the numeric columns Sentiment and Length\n",
    "col_idx_n = (df_features.columns == 'Sentiment') | (df_features.columns == 'Length')\n",
    "\n",
    "# Column index for TF-IDF features on the raw Text column with n-grams=1\n",
    "col_idx_w1 = df_features.columns.str.startswith('W1_')\n",
    "\n",
    "# Column index for TF-IDF features on the raw Text column with n-grams=2\n",
    "col_idx_w2 = df_features.columns.str.startswith('W2_')\n",
    "\n",
    "# Column index for TF-IDF features on the stemmed text with n-grams=1\n",
    "col_idx_ws = df_features.columns.str.startswith('WS_')\n",
    "\n",
    "# Column index for POS features\n",
    "col_idx_p = df_features.columns.str.startswith('P_')\n",
    "\n",
    "# Column index for NER labels\n",
    "col_idx_e = df_features.columns.str.startswith('E_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23154/4164159669.py:45: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  lstm_classifier = KerasClassifier(build_fn=create_lstm_model, epochs=5, batch_size=64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "282/282 [==============================] - 288s 1s/step - loss: 0.6813 - accuracy: 0.1081\n",
      "Epoch 2/5\n",
      "282/282 [==============================] - 258s 914ms/step - loss: 0.6790 - accuracy: 0.1104\n",
      "Epoch 3/5\n",
      "282/282 [==============================] - 418s 1s/step - loss: 0.6791 - accuracy: 0.1057\n",
      "Epoch 4/5\n",
      "282/282 [==============================] - 475s 2s/step - loss: 0.6795 - accuracy: 0.1056\n",
      "Epoch 5/5\n",
      "282/282 [==============================] - 496s 2s/step - loss: 0.6787 - accuracy: 0.1056\n",
      "141/141 [==============================] - 62s 428ms/step\n",
      "Accuracy: 11.15%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00      2926\n",
      "           0       0.11      1.00      0.20       502\n",
      "           1       0.00      0.00      0.00      1073\n",
      "\n",
      "    accuracy                           0.11      4501\n",
      "   macro avg       0.04      0.33      0.07      4501\n",
      "weighted avg       0.01      0.11      0.02      4501\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tor/anaconda3/envs/fact_check/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tor/anaconda3/envs/fact_check/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tor/anaconda3/envs/fact_check/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://zenodo.org/record/3609356/files/crowdsourced.csv?download=1\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocessing: Tokenize and pad the sequences\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_df[\"Text\"])\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(train_df[\"Text\"])\n",
    "X_test = tokenizer.texts_to_sequences(test_df[\"Text\"])\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen=256, padding=\"post\")\n",
    "X_test = pad_sequences(X_test, maxlen=256, padding=\"post\")\n",
    "\n",
    "y_train = train_df[\"Verdict\"]\n",
    "y_test = test_df[\"Verdict\"]\n",
    "\n",
    "# Define the Keras LSTM model\n",
    "def create_lstm_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=10000, output_dim=128, input_length=256))\n",
    "    model.add(LSTM(128, return_sequences=True))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(64))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# Instantiate and train the Keras LSTM classifier\n",
    "lstm_classifier = KerasClassifier(build_fn=create_lstm_model, epochs=5, batch_size=64)\n",
    "lstm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Test the classifier\n",
    "y_pred = lstm_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_classifier(x_features, y_labels, epochs=5, n_layers=1, n_neurons=10, verbose=0):\n",
    "    if not isinstance(n_neurons, list) and isinstance(n_neurons, int):\n",
    "        n_neurons = [n_neurons] * n_layers\n",
    "\n",
    "    # Setup the layers\n",
    "    model = tf.keras.Sequential()\n",
    "    for n in range(n_layers):\n",
    "        model.add(tf.keras.layers.Dense(n_neurons[n], activation='tanh', name=f'hidden{n+1}'))\n",
    "    model.add(tf.keras.layers.Dense(3, activation='sigmoid'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(x_features, y_labels, epochs=epochs, verbose=verbose)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_features.loc[idx_train, col_idx_w1 | col_idx_n].to_numpy()\n",
    "y = df_train['Verdict'].to_numpy() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf_classifier(X, y, epochs=5, n_layers=2, n_neurons=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_pred.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(np.argmax(y_pred, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FactCheck",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
