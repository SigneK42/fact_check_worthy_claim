{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT and Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import os\n",
    "import pickle\n",
    "import scipy as sp\n",
    "import sys\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "from fact_classification import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datafiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, df_crowdsourced, df_ground_truth = data_loading(local=True)\n",
    "df['Sentiment'] = df.Sentiment.fillna(df.Sentiment[df.Verdict == -1].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load features\n",
    "Load the features matrix that we generated in the `feature_generation.ipynb` notebook. This is a large sparse matrix so ww convert it to Compressed Sparse Row (CSR) format to avoid running out of memory when fitting our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with bz2.open('../results/df_features.bz2') as f:\n",
    "    df_features = pickle.load(f)\n",
    "\n",
    "# Convert to compressed sparse row matrix\n",
    "X = sp.sparse.csr_matrix(df_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data and generate indexes\n",
    "\n",
    "We split the dataset according to the instructions in the assignment, where data up until and including year 2008 will be used for training, and data after 2008 will be used for testing. Here we also generate indexes for the various feature sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test, idx_train = test_train_split(df)\n",
    "\n",
    "y = df['Verdict']\n",
    "y_train = df_train['Verdict']\n",
    "y_test = df_test['Verdict']\n",
    "\n",
    "X_train = X[idx_train]\n",
    "X_test = X[~idx_train]\n",
    "\n",
    "# Column index for the numeric columns Sentiment and Length\n",
    "col_idx_n = (df_features.columns == 'Sentiment') | (df_features.columns == 'Length')\n",
    "\n",
    "# Column index for TF-IDF features on the raw Text column with n-grams=1\n",
    "col_idx_w1 = df_features.columns.str.startswith('W1_')\n",
    "\n",
    "# Column index for TF-IDF features on the raw Text column with n-grams=2\n",
    "col_idx_w2 = df_features.columns.str.startswith('W2_')\n",
    "\n",
    "# Column index for TF-IDF features on the stemmed text with n-grams=1\n",
    "col_idx_ws = df_features.columns.str.startswith('WS_')\n",
    "\n",
    "# Column index for POS features\n",
    "col_idx_p = df_features.columns.str.startswith('P_')\n",
    "\n",
    "# Column index for NER labels\n",
    "col_idx_e = df_features.columns.str.startswith('E_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 14\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize_and_encode\u001b[39m(texts):\n\u001b[1;32m      5\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m      6\u001b[0m         texts,\n\u001b[1;32m      7\u001b[0m         max_length\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m         return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m     )\n\u001b[0;32m---> 14\u001b[0m X_train \u001b[39m=\u001b[39m tokenize_and_encode(train_df[\u001b[39m\"\u001b[39m\u001b[39mText\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mtolist())\n\u001b[1;32m     15\u001b[0m X_test \u001b[39m=\u001b[39m tokenize_and_encode(test_df[\u001b[39m\"\u001b[39m\u001b[39mText\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mtolist())\n\u001b[1;32m     17\u001b[0m y_train \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(train_df[\u001b[39m\"\u001b[39m\u001b[39mVerdict\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mtolist())\u001b[39m.\u001b[39madd(\u001b[39m1\u001b[39m)  \u001b[39m# Add 1 to shift labels from [-1, 0, 1] to [0, 1, 2]\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Preprocessing: Tokenize using the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize_and_encode(texts):\n",
    "    return tokenizer.batch_encode_plus(\n",
    "        texts,\n",
    "        max_length=256,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "X_train = tokenize_and_encode(train_df[\"Text\"].tolist())\n",
    "X_test = tokenize_and_encode(test_df[\"Text\"].tolist())\n",
    "\n",
    "y_train = torch.tensor(train_df[\"Verdict\"].tolist()).add(1)  # Add 1 to shift labels from [-1, 0, 1] to [0, 1, 2]\n",
    "y_test = torch.tensor(test_df[\"Verdict\"].tolist()).add(1)\n",
    "\n",
    "# Create a PyTorch dataset\n",
    "class CrowdsourcedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = CrowdsourcedDataset(X_train, y_train)\n",
    "test_dataset = CrowdsourcedDataset(X_test, y_test)\n",
    "\n",
    "# Instantiate the BERT model\n",
    "model = BertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3)\n",
    "\n",
    "# Define the Trainer and TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "# Train the BERT classifier\n",
    "trainer.train()\n",
    "\n",
    "# Test and evaluate the classifier\n",
    "y_pred = trainer.predict(test_dataset).predictions.argmax(axis=-1)\n",
    "\n",
    "# Shift labels back to original range [-1, 0, 1]\n",
    "y_pred = y_pred - 1\n",
    "y_test = y_test - 1\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fact_check",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
